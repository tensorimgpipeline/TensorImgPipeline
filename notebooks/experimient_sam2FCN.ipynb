{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sam2FCN\n",
    "\n",
    "This is the very first Expermient for Building the Repository based on real Scenario.\n",
    "\n",
    "## GOALS\n",
    "\n",
    "The Goal of this experiment is to create a first working pipeline structure, aswell as implementing sam2FCN.\n",
    "\n",
    "Goals to achieve:\n",
    "\n",
    "- [ ] Create Minimal MVP of the Experiment.\n",
    "- [ ] Create Builder and Sequence Based Architecture.\n",
    "- [ ] Implement the minimal MVP in config form to Build the Pipeline and execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP Experiment with Mars Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.patches import Rectangle\n",
    "from segment_anything import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0][\"segmentation\"].shape[0], sorted_anns[0][\"segmentation\"].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann[\"segmentation\"]\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bboxes(bboxes):\n",
    "    if len(bboxes) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        xmin, ymin, xmax, ymax, class_label = bbox\n",
    "\n",
    "        # Draw rectangle\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        rect = Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label\n",
    "        ax.text(\n",
    "            xmin,\n",
    "            ymin - 5,\n",
    "            class_label,\n",
    "            color=\"red\",\n",
    "            fontsize=12,\n",
    "            weight=\"bold\",\n",
    "            bbox={\"facecolor\": \"white\", \"alpha\": 0.7, \"edgecolor\": \"none\"},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BndBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    def to_tensor(self, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts the bounding box to a PyTorch tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (4,) representing [xmin, ymin, xmax, ymax].\n",
    "        \"\"\"\n",
    "        if device:\n",
    "            return torch.as_tensor([self.xmin, self.ymin, self.xmax, self.ymax], dtype=torch.int64, device=device)\n",
    "        return torch.as_tensor([self.xmin, self.ymin, self.xmax, self.ymax], dtype=torch.int64)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ObjectAnnotation:\n",
    "    name: str\n",
    "    pose: str\n",
    "    truncated: int\n",
    "    difficult: int\n",
    "    bndbox: BndBox\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ImageAnnotation:\n",
    "    folder: str\n",
    "    filename: str\n",
    "    width: int\n",
    "    height: int\n",
    "    depth: int\n",
    "    objects: list[ObjectAnnotation] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_voc_xml(xml_file: Path) -> ImageAnnotation:\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Type safety: Check for None explicitly and provide fallbacks\n",
    "    folder = root.findtext(\"folder\") or \"\"\n",
    "    filename = root.findtext(\"filename\") or \"\"\n",
    "\n",
    "    # Ensure 'size' exists\n",
    "    size = root.find(\"size\")\n",
    "    if size is None:\n",
    "        raise ValueError(\"Missing <size> element in the XML file.\")\n",
    "\n",
    "    # Extract size details safely\n",
    "    width = int(size.findtext(\"width\") or 0)\n",
    "    height = int(size.findtext(\"height\") or 0)\n",
    "    depth = int(size.findtext(\"depth\") or 0)\n",
    "\n",
    "    # Parse objects safely\n",
    "    objects = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\") or \"\"\n",
    "        pose = obj.findtext(\"pose\") or \"Unspecified\"\n",
    "        truncated = int(obj.findtext(\"truncated\") or 0)\n",
    "        difficult = int(obj.findtext(\"difficult\") or 0)\n",
    "\n",
    "        bndbox = obj.find(\"bndbox\")\n",
    "        if bndbox is None:\n",
    "            raise ValueError(\"Missing <bndbox> element for an object in the XML file.\")\n",
    "\n",
    "        bbox = BndBox(\n",
    "            xmin=int(bndbox.findtext(\"xmin\") or 0),\n",
    "            ymin=int(bndbox.findtext(\"ymin\") or 0),\n",
    "            xmax=int(bndbox.findtext(\"xmax\") or 0),\n",
    "            ymax=int(bndbox.findtext(\"ymax\") or 0),\n",
    "        )\n",
    "        objects.append(ObjectAnnotation(name, pose, truncated, difficult, bbox))\n",
    "\n",
    "    return ImageAnnotation(folder, filename, width, height, depth, objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_masks_with_priority(stacked_masks):\n",
    "    \"\"\"\n",
    "    Merge stacked binary masks where higher indices have priority\n",
    "\n",
    "    Args:\n",
    "        stacked_masks (torch.Tensor): Shape (N, H, W) where N is number of masks\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Shape (H, W) merged mask\n",
    "    \"\"\"\n",
    "    result = torch.zeros_like(stacked_masks[0])\n",
    "    for mask in stacked_masks:\n",
    "        result[mask > 0] = mask[mask > 0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"RLB_581357762EDR_F0701752RHAZ00337M1\"\n",
    "image = cv2.imread(f\"../data/datasets/mars/Images/{data_file}.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "annotation_file = f\"../data/datasets/mars/Annotations/{data_file}.xml\"\n",
    "annotations = parse_voc_xml(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"../data/models/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need the additional BBoxes from the pascalvoc annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "for obj in annotations.objects:\n",
    "    bboxes.append([obj.bndbox.xmin, obj.bndbox.ymin, obj.bndbox.xmax, obj.bndbox.ymax, obj.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "show_bboxes(bboxes)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator_2 = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.86,\n",
    "    stability_score_thresh=0.92,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks2 = mask_generator_2.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "show_bboxes(bboxes)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this Hole Image Classification we could start and give the segments labels from the Bounding Boxes.\n",
    "\n",
    "We have following Ideas in mind:\n",
    "\n",
    "```\n",
    "For Each BBox\n",
    "    For Each Segment:\n",
    "        IOU(Bbox, Segment)\n",
    "        VerticalDist(UpperPixel(Segment), BBox)\n",
    "        VerticalDist(LowerPixel(Segment), BBox)\n",
    "        HorizontalDist(LeftPixel(Segment), BBox)\n",
    "        HorizontalDist(RightPixel(Segment), BBox)\n",
    "    SelectSegementWithBestMetrics()\n",
    "```\n",
    "\n",
    "Additional we could do something similiar From the Demo.\n",
    "Since we already have BBoxes, we could use the BBoxes to create ROI which we use to calculate one segment with for this ROI.\n",
    "\n",
    "Looking into that, it sounds very Promising. I ferify the workflow, they showed for multiple bboxes, with the Predictor class.\n",
    "\n",
    "### Batched Prompt Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = torch.stack([obj.bndbox.to_tensor(device=predictor.device) for obj in annotations.objects])\n",
    "input_classes = [obj.name for obj in annotations.objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Prompt over all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)\n",
    "\n",
    "dataset_location = Path(\"../data/datasets/mars/\")\n",
    "\n",
    "dataset_location / \"Images\"\n",
    "\n",
    "total_iterations = len(list((dataset_location / \"Images\").glob(\"*.png\")))\n",
    "\n",
    "# Calculate subplot grid dimensions\n",
    "rows = int(np.ceil(np.sqrt(total_iterations)))\n",
    "cols = int(np.ceil(total_iterations / rows))\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10 * cols, 10 * rows))\n",
    "axes = axes.ravel()  # Flatten to 1D array for easy indexing\n",
    "\n",
    "for idx, image_file in enumerate((dataset_location / \"Images\").glob(\"*.png\")):\n",
    "    ax = axes[idx]\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    try:\n",
    "        annotations = parse_voc_xml(dataset_location / f\"Annotations/{image_file.stem}.xml\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No Annotations found for {image_file.stem}\")\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    input_boxes = torch.stack([obj.bndbox.to_tensor(device=predictor.device) for obj in annotations.objects])\n",
    "    input_classes = [obj.name for obj in annotations.objects]\n",
    "\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "    masks, _, _ = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    ax.imshow(image)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    for mask in masks:\n",
    "        show_mask(mask.cpu().numpy(), ax, random_color=True)\n",
    "    for box in input_boxes:\n",
    "        show_box(box.cpu().numpy(), ax)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(total_iterations, rows * cols):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now with all 100 first labeled Images of the mars data, we can See some promising results.\n",
    "NOw lets ivenstigate one Image and get the combined masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"RLB_581357762EDR_F0701752RHAZ00337M1\"\n",
    "image = cv2.imread(f\"../data/datasets/mars/Images/{data_file}.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)\n",
    "\n",
    "input_boxes = torch.stack([obj.bndbox.to_tensor(device=predictor.device) for obj in annotations.objects])\n",
    "input_classes = [obj.name for obj in annotations.objects]\n",
    "\n",
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "show_bboxes(bboxes)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[idx].cpu().numpy(), plt.gca(), random_color=True)\n",
    "show_bboxes([\n",
    "    bboxes[idx],\n",
    "])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_as_idx = {\"bg\": 0, \"sky\": 1, \"bedrock\": 2, \"robot\": 6}\n",
    "\n",
    "classes_of_masks = torch.tensor([classes_as_idx[obj.name] for obj in annotations.objects])\n",
    "\n",
    "t = masks.clone()\n",
    "r = t * classes_of_masks.unsqueeze(1).unsqueeze(1).unsqueeze(1).to(\"cuda:0\")\n",
    "r[2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classed_masks = masks * classes_of_masks.unsqueeze(1).unsqueeze(1).unsqueeze(1).to(\"cuda:0\")\n",
    "combined_masks = merge_masks_with_priority(classed_masks)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(combined_masks.cpu().numpy(), plt.gca(), random_color=True)\n",
    "show_bboxes(bboxes)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Border Around Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def add_border(masks: torch.Tensor, border_value=255):\n",
    "    # Ensure mask is binary (0 or 1), assuming mask values are already 0 and 1\n",
    "    masks = masks.float()\n",
    "\n",
    "    # Perform max pooling to create the border effect\n",
    "    dilated = F.max_pool2d(masks.unsqueeze(0), kernel_size=5, stride=1, padding=2).squeeze()\n",
    "\n",
    "    # Border is where dilated is 1 but mask is 0\n",
    "    border = (dilated - masks).bool() * border_value\n",
    "\n",
    "    # Add the border to the mask\n",
    "    result = masks + border\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "mask = torch.zeros((2, 10, 10), dtype=torch.uint8)\n",
    "mask[0, 3:7, 3:7] = 1  # Example square mask\n",
    "mask[1, 2:8, 2] = 4  # Example square mask\n",
    "mask[1, 4:9, 3] = 4  # Example square mask\n",
    "mask[1, 5:9, 4] = 4  # Example square mask\n",
    "mask[1, 5:10, 5] = 4  # Example square mask\n",
    "mask[1, 3:9, 6] = 4  # Example square mask\n",
    "mask[1, 2:7, 7] = 4  # Example square mask\n",
    "\n",
    "bordered_mask = add_border(mask)\n",
    "print(bordered_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with image erode and dilate (openclose approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(image, padding=1):\n",
    "    \"\"\"Reads image with shape = (N,C,B,W)\"\"\"\n",
    "    kernel_size = 2 * padding + 1\n",
    "    return F.max_pool2d(image, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "\n",
    "\n",
    "def erode(image, padding=1):\n",
    "    \"\"\"Reads image with shape = (N,C,B,W)\"\"\"\n",
    "    kernel_size = 2 * padding + 1\n",
    "    return -F.max_pool2d(-image, kernel_size=kernel_size, stride=1, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Test image\n",
    "image = torch.zeros((7, 7), dtype=int)\n",
    "image[2:5, 2:5] = 1\n",
    "image[4, 4] = 2\n",
    "image[2, 3] = 3\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=\"Greys\", vmin=image.min(), vmax=image.max(), origin=\"lower\")\n",
    "plt.title(\"Original image\")\n",
    "\n",
    "# PyTorch\n",
    "image_tensor = torch.tensor(image, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "dilated_image_pytorch = dilate(image_tensor, padding=1).squeeze().squeeze()\n",
    "plt.figure()\n",
    "plt.imshow(dilated_image_pytorch.cpu().numpy(), cmap=\"Greys\", vmin=image.min(), vmax=image.max(), origin=\"lower\")\n",
    "plt.title(\"Dilated image - PyTorch\")\n",
    "\n",
    "eroded_image_pytorch = erode(image_tensor, padding=1).squeeze().squeeze()\n",
    "plt.figure()\n",
    "plt.imshow(eroded_image_pytorch.cpu().numpy(), cmap=\"Greys\", vmin=image.min(), vmax=image.max(), origin=\"lower\")\n",
    "plt.title(\"Eroded image - PyTorch\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
